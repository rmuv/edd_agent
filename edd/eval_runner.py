import json
import re
from typing import Dict, Any, List, Tuple
from difflib import SequenceMatcher


class EvalRunner:
    """
    Comprehensive evaluation runner that validates agent output against expected results
    and checks all thresholds defined in the eval records.
    """

    def __init__(self):
        self.results = []

    def run_eval(self, eval_record: Dict[str, Any], agent_output: Dict[str, Any], metrics: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Runs comprehensive evaluation on a single task.
        
        Args:
            eval_record: The eval record with expected output, thresholds, and assertions
            agent_output: The actual output from the agent
            metrics: Optional performance metrics (latency_ms, token_counts, etc.)
        
        Returns:
            Dict with evaluation results including pass/fail status and detailed findings
        """
        task_id = eval_record.get("task_id", "unknown")
        expected = eval_record.get("expected", {})
        thresholds = eval_record.get("thresholds", {})
        assertions = eval_record.get("assertions", {})
        
        findings = {
            "task_id": task_id,
            "overall_status": "passed",
            "checks": {
                "output_match": [],
                "thresholds": [],
                "assertions": [],
                "constraints": []
            },
            "scores": {}
        }
        
        # 1. Output Match Validation
        output_checks = self._validate_output_match(expected, agent_output)
        findings["checks"]["output_match"] = output_checks
        
        # 2. Threshold Validation
        if metrics:
            threshold_checks = self._validate_thresholds(thresholds, metrics)
            findings["checks"]["thresholds"] = threshold_checks
        
        # 3. Assertion Validation
        assertion_checks = self._validate_assertions(assertions, agent_output, eval_record)
        findings["checks"]["assertions"] = assertion_checks
        
        # 4. Constraint Validation
        constraint_checks = self._validate_constraints(assertions.get("constraints", {}), agent_output, eval_record)
        findings["checks"]["constraints"] = constraint_checks
        
        # 5. Calculate Scores
        findings["scores"] = self._calculate_scores(expected, agent_output, eval_record)
        
        # Determine overall status
        all_checks = (
            output_checks + 
            findings["checks"]["thresholds"] + 
            assertion_checks + 
            constraint_checks
        )
        
        if any(check["status"] == "failed" for check in all_checks):
            findings["overall_status"] = "failed"
        elif any(check["status"] == "warning" for check in all_checks):
            findings["overall_status"] = "passed_with_warnings"
        
        return findings

    def _validate_output_match(self, expected: Dict, actual: Dict) -> List[Dict]:
        """Validates that the actual output matches the expected output."""
        checks = []
        
        exp_msg = expected.get("next_message", {}) or {}
        act_msg = actual.get("next_message", {}) or {}
        
        # Channel check
        exp_channel = exp_msg.get("channel")
        act_channel = act_msg.get("channel")
        checks.append({
            "name": "channel_match",
            "status": "passed" if exp_channel == act_channel else "failed",
            "expected": exp_channel,
            "actual": act_channel,
            "message": f"Channel: expected '{exp_channel}', got '{act_channel}'"
        })
        
        # Subject check (for emails)
        if exp_channel == "email":
            exp_subject = exp_msg.get("subject")
            act_subject = act_msg.get("subject")
            similarity = self._calculate_similarity(exp_subject or "", act_subject or "")
            checks.append({
                "name": "subject_match",
                "status": "passed" if similarity > 0.85 else "warning" if similarity > 0.7 else "failed",
                "expected": exp_subject,
                "actual": act_subject,
                "similarity": similarity,
                "message": f"Subject similarity: {similarity:.2%}"
            })
        
        # Body check
        if exp_msg.get("body") is not None and act_msg.get("body") is not None:
            exp_body = exp_msg.get("body", "")
            act_body = act_msg.get("body", "")
            similarity = self._calculate_similarity(exp_body, act_body)
            checks.append({
                "name": "body_similarity",
                "status": "passed" if similarity > 0.85 else "warning" if similarity > 0.7 else "failed",
                "expected": exp_body[:100] + "..." if len(exp_body) > 100 else exp_body,
                "actual": act_body[:100] + "..." if len(act_body) > 100 else act_body,
                "similarity": similarity,
                "message": f"Body similarity: {similarity:.2%}"
            })
        elif exp_msg.get("body") is None and act_msg.get("body") is None:
            checks.append({
                "name": "body_null_match",
                "status": "passed",
                "message": "Both bodies are null (no message sent)"
            })
        
        # CTA check
        exp_cta = exp_msg.get("cta", {}) or {}
        act_cta = act_msg.get("cta", {}) or {}
        
        if exp_cta and act_cta:
            cta_type_match = exp_cta.get("type") == act_cta.get("type")
            checks.append({
                "name": "cta_type_match",
                "status": "passed" if cta_type_match else "failed",
                "expected": exp_cta.get("type"),
                "actual": act_cta.get("type"),
                "message": f"CTA type: expected '{exp_cta.get('type')}', got '{act_cta.get('type')}'"
            })
        
        # Next action check
        exp_action = expected.get("next_action", {}) or {}
        act_action = actual.get("next_action", {}) or {}
        
        action_type_match = exp_action.get("type") == act_action.get("type")
        checks.append({
            "name": "next_action_type_match",
            "status": "passed" if action_type_match else "failed",
            "expected": exp_action.get("type"),
            "actual": act_action.get("type"),
            "message": f"Next action: expected '{exp_action.get('type')}', got '{act_action.get('type')}'"
        })
        
        return checks

    def _validate_thresholds(self, thresholds: Dict, metrics: Dict) -> List[Dict]:
        """Validates that performance metrics meet the defined thresholds."""
        checks = []
        
        # Latency threshold
        if "p95_latency_ms" in thresholds:
            threshold_ms = thresholds["p95_latency_ms"]
            actual_ms = metrics.get("latency_ms", 0)
            passed = actual_ms <= threshold_ms
            checks.append({
                "name": "latency_threshold",
                "status": "passed" if passed else "failed",
                "threshold": threshold_ms,
                "actual": actual_ms,
                "message": f"Latency: {actual_ms}ms (threshold: {threshold_ms}ms)"
            })
        
        # Personalization score
        if "personalization_score_min" in thresholds:
            min_score = thresholds["personalization_score_min"]
            actual_score = metrics.get("personalization_score", 0)
            passed = actual_score >= min_score
            checks.append({
                "name": "personalization_score_threshold",
                "status": "passed" if passed else "warning",
                "threshold": min_score,
                "actual": actual_score,
                "message": f"Personalization: {actual_score:.2f} (min: {min_score})"
            })
        
        # Locale accuracy
        if "locale_accuracy_min" in thresholds:
            min_accuracy = thresholds["locale_accuracy_min"]
            actual_accuracy = metrics.get("locale_accuracy", 0)
            passed = actual_accuracy >= min_accuracy
            checks.append({
                "name": "locale_accuracy_threshold",
                "status": "passed" if passed else "failed",
                "threshold": min_accuracy,
                "actual": actual_accuracy,
                "message": f"Locale accuracy: {actual_accuracy:.2f} (min: {min_accuracy})"
            })
        
        # Safety violations
        if "safety_violations_max" in thresholds:
            max_violations = thresholds["safety_violations_max"]
            actual_violations = metrics.get("safety_violations", 0)
            passed = actual_violations <= max_violations
            checks.append({
                "name": "safety_violations_threshold",
                "status": "passed" if passed else "failed",
                "threshold": max_violations,
                "actual": actual_violations,
                "message": f"Safety violations: {actual_violations} (max: {max_violations})"
            })
        
        return checks

    def _validate_assertions(self, assertions: Dict, agent_output: Dict, eval_record: Dict) -> List[Dict]:
        """Validates required states and assertions."""
        checks = []
        
        required_states = assertions.get("required_states", [])
        
        # For now, we'll check basic states that we can infer
        for state in required_states:
            if state == "consent_verified":
                # Check if agent respected consent
                consent = eval_record.get("consent", {})
                channel = agent_output.get("next_message", {}).get("channel")
                
                if channel == "none":
                    # Agent chose not to send, which is correct if no consent
                    all_denied = not any(consent.values())
                    checks.append({
                        "name": f"assertion_{state}",
                        "status": "passed" if all_denied else "warning",
                        "message": f"Consent verified: no message sent (all channels denied: {all_denied})"
                    })
                elif channel == "sms":
                    checks.append({
                        "name": f"assertion_{state}",
                        "status": "passed" if consent.get("sms_opt_in") else "failed",
                        "message": f"Consent verified: SMS channel (opt-in: {consent.get('sms_opt_in')})"
                    })
                elif channel == "email":
                    checks.append({
                        "name": f"assertion_{state}",
                        "status": "passed" if consent.get("email_opt_in") else "failed",
                        "message": f"Consent verified: Email channel (opt-in: {consent.get('email_opt_in')})"
                    })
            
            elif state in ["fair_housing_check_passed", "brand_style_applied", "renewal_offer_loaded"]:
                # These would require actual tool call logs or more context
                checks.append({
                    "name": f"assertion_{state}",
                    "status": "passed",
                    "message": f"Assertion '{state}' assumed passed (requires tool trace validation)"
                })
        
        return checks

    def _validate_constraints(self, constraints: Dict, agent_output: Dict, eval_record: Dict) -> List[Dict]:
        """Validates constraints like no PII leak, opt-out instructions, etc."""
        checks = []
        
        msg = agent_output.get("next_message", {}) or {}
        body = msg.get("body") or ""
        body_lower = body.lower() if body else ""
        channel = msg.get("channel")
        
        # Opt-out instructions
        if constraints.get("include_opt_out_instructions") and channel != "none":
            if channel == "sms":
                has_stop = "stop" in body_lower
                checks.append({
                    "name": "constraint_opt_out_sms",
                    "status": "passed" if has_stop else "failed",
                    "message": f"SMS opt-out instruction: {'STOP found' if has_stop else 'STOP missing'}"
                })
            elif channel == "email":
                has_opt_out = any(phrase in body_lower for phrase in ["opt", "unsubscribe", "reply stop"])
                checks.append({
                    "name": "constraint_opt_out_email",
                    "status": "passed" if has_opt_out else "failed",
                    "message": f"Email opt-out instruction: {'found' if has_opt_out else 'missing'}"
                })
        
        # Locale applied (language check)
        if constraints.get("locale_applied"):
            language = eval_record.get("input", {}).get("language", "en")
            if language == "es":
                # Check for Spanish indicators
                spanish_indicators = ["hola", "gracias", "quieres", "responde", "para"]
                has_spanish = any(word in body_lower for word in spanish_indicators)
                checks.append({
                    "name": "constraint_locale_applied",
                    "status": "passed" if has_spanish else "failed",
                    "message": f"Spanish locale applied: {has_spanish}"
                })
        
        # Respect consent
        if constraints.get("respect_consent"):
            consent = eval_record.get("consent", {})
            all_denied = not any(consent.values())
            no_message_sent = channel == "none" or msg.get("body") is None
            
            checks.append({
                "name": "constraint_respect_consent",
                "status": "passed" if (all_denied and no_message_sent) or not all_denied else "failed",
                "message": f"Consent respected: all denied={all_denied}, no message={no_message_sent}"
            })
        
        # Primary CTA
        if constraints.get("primary_cta"):
            expected_cta = constraints["primary_cta"]
            actual_cta = msg.get("cta", {}).get("type")
            checks.append({
                "name": "constraint_primary_cta",
                "status": "passed" if expected_cta == actual_cta else "warning",
                "expected": expected_cta,
                "actual": actual_cta,
                "message": f"Primary CTA: expected '{expected_cta}', got '{actual_cta}'"
            })
        
        return checks

    def _calculate_scores(self, expected: Dict, actual: Dict, eval_record: Dict) -> Dict[str, float]:
        """Calculates various quality scores."""
        scores = {}
        
        exp_msg = expected.get("next_message", {}) or {}
        act_msg = actual.get("next_message", {}) or {}
        
        # Overall output similarity
        exp_body = exp_msg.get("body", "")
        act_body = act_msg.get("body", "")
        scores["body_similarity"] = self._calculate_similarity(exp_body, act_body)
        
        # Personalization score (check for profile usage)
        profile = eval_record.get("input", {}).get("profile", {})
        personalization_points = 0
        max_points = 0
        
        act_body_lower = act_body.lower() if act_body else ""
        
        if profile.get("first_name"):
            max_points += 1
            if profile["first_name"].lower() in act_body_lower:
                personalization_points += 1
        
        if profile.get("city_interest"):
            max_points += 1
            # City might not always be in message, so this is optional
        
        if profile.get("amenity_interest"):
            max_points += 1
            amenities = profile["amenity_interest"]
            if any(amenity.lower() in act_body_lower for amenity in amenities):
                personalization_points += 1
        
        if eval_record.get("input", {}).get("unit"):
            max_points += 1
            unit = eval_record["input"]["unit"]
            if unit in (act_msg.get("body") or ""):
                personalization_points += 1
        
        scores["personalization_score"] = personalization_points / max_points if max_points > 0 else 1.0
        
        # Locale accuracy (for non-English)
        language = eval_record.get("input", {}).get("language", "en")
        if language == "es" and act_body:
            spanish_words = len(re.findall(r'\b(hola|gracias|por|tu|para|quieres|responde)\b', act_body_lower))
            total_words = len(act_body.split())
            scores["locale_accuracy"] = min(1.0, spanish_words / max(total_words * 0.3, 1))
        else:
            scores["locale_accuracy"] = 1.0
        
        # Safety score (no violations assumed for now)
        scores["safety_violations"] = 0
        
        return scores

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculates similarity ratio between two strings."""
        if not text1 and not text2:
            return 1.0
        if not text1 or not text2:
            return 0.0
        return SequenceMatcher(None, text1, text2).ratio()

    def generate_report(self, findings_list: List[Dict]) -> str:
        """Generates a human-readable report from evaluation findings."""
        report = []
        report.append("=" * 80)
        report.append("EVALUATION REPORT")
        report.append("=" * 80)
        report.append("")
        
        passed = sum(1 for f in findings_list if f["overall_status"] == "passed")
        warnings = sum(1 for f in findings_list if f["overall_status"] == "passed_with_warnings")
        failed = sum(1 for f in findings_list if f["overall_status"] == "failed")
        
        report.append(f"Total Tasks: {len(findings_list)}")
        report.append(f"âœ… Passed: {passed}")
        report.append(f"âš ï¸  Passed with Warnings: {warnings}")
        report.append(f"âŒ Failed: {failed}")
        report.append("")
        
        for finding in findings_list:
            task_id = finding["task_id"]
            status = finding["overall_status"]
            status_emoji = "âœ…" if status == "passed" else "âš ï¸" if status == "passed_with_warnings" else "âŒ"
            
            report.append("-" * 80)
            report.append(f"{status_emoji} Task: {task_id} ({status.upper()})")
            report.append("-" * 80)
            
            # Output match checks
            if finding["checks"]["output_match"]:
                report.append("\nğŸ“‹ Output Match Checks:")
                for check in finding["checks"]["output_match"]:
                    status_symbol = "âœ“" if check["status"] == "passed" else "âš " if check["status"] == "warning" else "âœ—"
                    report.append(f"  {status_symbol} {check['message']}")
            
            # Threshold checks
            if finding["checks"]["thresholds"]:
                report.append("\nâ±ï¸  Threshold Checks:")
                for check in finding["checks"]["thresholds"]:
                    status_symbol = "âœ“" if check["status"] == "passed" else "âš " if check["status"] == "warning" else "âœ—"
                    report.append(f"  {status_symbol} {check['message']}")
            
            # Assertion checks
            if finding["checks"]["assertions"]:
                report.append("\nğŸ”’ Assertion Checks:")
                for check in finding["checks"]["assertions"]:
                    status_symbol = "âœ“" if check["status"] == "passed" else "âš " if check["status"] == "warning" else "âœ—"
                    report.append(f"  {status_symbol} {check['message']}")
            
            # Constraint checks
            if finding["checks"]["constraints"]:
                report.append("\nâš–ï¸  Constraint Checks:")
                for check in finding["checks"]["constraints"]:
                    status_symbol = "âœ“" if check["status"] == "passed" else "âš " if check["status"] == "warning" else "âœ—"
                    report.append(f"  {status_symbol} {check['message']}")
            
            # Scores
            if finding["scores"]:
                report.append("\nğŸ“Š Scores:")
                for score_name, score_value in finding["scores"].items():
                    report.append(f"  â€¢ {score_name}: {score_value:.2%}")
            
            report.append("")
        
        report.append("=" * 80)
        return "\n".join(report)
